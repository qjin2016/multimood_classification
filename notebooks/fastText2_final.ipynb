{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is used for developing modified fastText model -- fastText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Embedding, Activation, GlobalAveragePooling1D\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3561, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data\n",
    "empathies = pd.read_csv('./empathies.csv')\n",
    "msg = pd.read_csv('./msg_fasttext.csv')\n",
    "msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "msg = msg.drop(msg.columns[0:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use NLTK TweetTokenizer to tokenize data. The TweetTokenizer is able to recognize emoji\n",
    "sentences_tokens = []\n",
    "for m in msg['message']:\n",
    "    tokens = TweetTokenizer().tokenize(m.lower())\n",
    "    sentences_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obain a set of all the tokens\n",
    "token_set = []\n",
    "for sen in sentences_tokens:\n",
    "    for tok in sen:\n",
    "        if tok not in token_set:\n",
    "            token_set.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indexing tokens\n",
    "token_indices = dict((c, i) for i, c in enumerate(token_set))\n",
    "indices_token = dict((i, c) for i, c in enumerate(token_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform original input data into token index\n",
    "max_len = 128\n",
    "X = []\n",
    "\n",
    "for i, sen in enumerate(sentences_tokens):\n",
    "    sen2token = []\n",
    "    for j, tok in enumerate(sen):\n",
    "        if j < max_len:\n",
    "            sen2token.append(token_indices[tok])\n",
    "    X.append(sen2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code borrowed from https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n",
    "def create_ngram_set(input_list, ngram_value):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code borrowed from https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n",
    "def add_ngram(sequences, token_indice, ngram_range):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list) - ngram_range + 1):\n",
    "            for ngram_value in range(2, ngram_range + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_range = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 3-gram features\n",
      "Average sequence length: 11\n"
     ]
    }
   ],
   "source": [
    "# code modified from https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n",
    "max_features = len(token_set)\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in X:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice2 = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token2 = {token_indice2[k]: k for k in token_indice2}\n",
    "    \n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token2.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    X_ft = add_ngram(X, token_indice2, ngram_range)\n",
    "    print('Average sequence length: {}'.format(np.mean(list(map(len, X_ft)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padding input data\n",
    "X_padding = pad_sequences(X, maxlen = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get training and testing data\n",
    "train_padding = X_padding[:2500]\n",
    "test_padding = X_padding[2500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get ground truth\n",
    "sentiments = np.zeros((msg.shape[0], empathies.shape[0]), dtype = np.int64)\n",
    "i = 0\n",
    "for emps in msg['empathy']:\n",
    "    for emp in emps.split(','):\n",
    "        emp = emp.strip()\n",
    "        try:\n",
    "            idx = empathies[empathies['empathy'] == emp].index[0]\n",
    "            sentiments[i][idx] = 1\n",
    "        except:\n",
    "            print(emp)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = sentiments[:2500]\n",
    "test_y = sentiments[2500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fastText model\n",
    "# code modified from https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    EMBEDDING_DIM,\n",
    "                    input_length = 128))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.05)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 100)          1958500   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "=================================================================\n",
      "Total params: 1,964,964\n",
      "Trainable params: 1,964,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 1061 samples\n",
      "Epoch 1/40\n",
      "2500/2500 [==============================] - 1s - loss: 5.2355 - acc: 0.0292 - val_loss: 5.1750 - val_acc: 0.0377\n",
      "Epoch 2/40\n",
      "2500/2500 [==============================] - 0s - loss: 5.0094 - acc: 0.0604 - val_loss: 5.0479 - val_acc: 0.0424\n",
      "Epoch 3/40\n",
      "2500/2500 [==============================] - 0s - loss: 4.8626 - acc: 0.0828 - val_loss: 4.8807 - val_acc: 0.1423\n",
      "Epoch 4/40\n",
      "2500/2500 [==============================] - 1s - loss: 4.5875 - acc: 0.2020 - val_loss: 4.6183 - val_acc: 0.1960\n",
      "Epoch 5/40\n",
      "2500/2500 [==============================] - 0s - loss: 4.1427 - acc: 0.3212 - val_loss: 4.2010 - val_acc: 0.3421\n",
      "Epoch 6/40\n",
      "2500/2500 [==============================] - 0s - loss: 3.5405 - acc: 0.5000 - val_loss: 3.6931 - val_acc: 0.4062\n",
      "Epoch 7/40\n",
      "2500/2500 [==============================] - 0s - loss: 2.8912 - acc: 0.5868 - val_loss: 3.2299 - val_acc: 0.4760\n",
      "Epoch 8/40\n",
      "2500/2500 [==============================] - 1s - loss: 2.3772 - acc: 0.6684 - val_loss: 2.8869 - val_acc: 0.5580\n",
      "Epoch 9/40\n",
      "2500/2500 [==============================] - 0s - loss: 1.9624 - acc: 0.7168 - val_loss: 2.6075 - val_acc: 0.6098\n",
      "Epoch 10/40\n",
      "2500/2500 [==============================] - 0s - loss: 1.6673 - acc: 0.7660 - val_loss: 2.4903 - val_acc: 0.6107\n",
      "Epoch 11/40\n",
      "2500/2500 [==============================] - 0s - loss: 1.4373 - acc: 0.7784 - val_loss: 2.3239 - val_acc: 0.6381\n",
      "Epoch 12/40\n",
      "2500/2500 [==============================] - 0s - loss: 1.2823 - acc: 0.7960 - val_loss: 2.2828 - val_acc: 0.6541\n",
      "Epoch 13/40\n",
      "2500/2500 [==============================] - 0s - loss: 1.1666 - acc: 0.8004 - val_loss: 2.2501 - val_acc: 0.6305\n",
      "Epoch 14/40\n",
      "2500/2500 [==============================] - 1s - loss: 1.0562 - acc: 0.8152 - val_loss: 2.1834 - val_acc: 0.6484\n",
      "Epoch 15/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.9751 - acc: 0.8188 - val_loss: 2.1864 - val_acc: 0.6503\n",
      "Epoch 16/40\n",
      "2500/2500 [==============================] - 1s - loss: 0.9258 - acc: 0.8144 - val_loss: 2.2017 - val_acc: 0.6484\n",
      "Epoch 17/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.8723 - acc: 0.8132 - val_loss: 2.1392 - val_acc: 0.6588\n",
      "Epoch 18/40\n",
      "2500/2500 [==============================] - 1s - loss: 0.8271 - acc: 0.8164 - val_loss: 2.1953 - val_acc: 0.6541\n",
      "Epoch 19/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7978 - acc: 0.8192 - val_loss: 2.1631 - val_acc: 0.6428\n",
      "Epoch 20/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7665 - acc: 0.8244 - val_loss: 2.1453 - val_acc: 0.6569\n",
      "Epoch 21/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7562 - acc: 0.8324 - val_loss: 2.1929 - val_acc: 0.6664\n",
      "Epoch 22/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7399 - acc: 0.8312 - val_loss: 2.2055 - val_acc: 0.6418\n",
      "Epoch 23/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7471 - acc: 0.8244 - val_loss: 2.2536 - val_acc: 0.6758\n",
      "Epoch 24/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7396 - acc: 0.8392 - val_loss: 2.2190 - val_acc: 0.6343\n",
      "Epoch 25/40\n",
      "2500/2500 [==============================] - 1s - loss: 0.7123 - acc: 0.8224 - val_loss: 2.2155 - val_acc: 0.6607\n",
      "Epoch 26/40\n",
      "2500/2500 [==============================] - 1s - loss: 0.7014 - acc: 0.8404 - val_loss: 2.2517 - val_acc: 0.6475\n",
      "Epoch 27/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7196 - acc: 0.8340 - val_loss: 2.2480 - val_acc: 0.6428\n",
      "Epoch 28/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.7025 - acc: 0.8224 - val_loss: 2.2277 - val_acc: 0.6494\n",
      "Epoch 29/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6949 - acc: 0.8356 - val_loss: 2.2723 - val_acc: 0.6522\n",
      "Epoch 30/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6887 - acc: 0.8356 - val_loss: 2.2454 - val_acc: 0.6475\n",
      "Epoch 31/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6836 - acc: 0.8324 - val_loss: 2.2604 - val_acc: 0.6607\n",
      "Epoch 32/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6682 - acc: 0.8360 - val_loss: 2.2637 - val_acc: 0.6418\n",
      "Epoch 33/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6597 - acc: 0.8300 - val_loss: 2.2935 - val_acc: 0.6598\n",
      "Epoch 34/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6538 - acc: 0.8424 - val_loss: 2.2707 - val_acc: 0.6381\n",
      "Epoch 35/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6480 - acc: 0.8300 - val_loss: 2.2804 - val_acc: 0.6362\n",
      "Epoch 36/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6551 - acc: 0.8304 - val_loss: 2.3015 - val_acc: 0.6626\n",
      "Epoch 37/40\n",
      "2500/2500 [==============================] - 1s - loss: 0.6506 - acc: 0.8420 - val_loss: 2.3498 - val_acc: 0.6456\n",
      "Epoch 38/40\n",
      "2500/2500 [==============================] - 1s - loss: 0.6549 - acc: 0.8380 - val_loss: 2.3553 - val_acc: 0.6352\n",
      "Epoch 39/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6452 - acc: 0.8432 - val_loss: 2.3680 - val_acc: 0.6287\n",
      "Epoch 40/40\n",
      "2500/2500 [==============================] - 0s - loss: 0.6502 - acc: 0.8284 - val_loss: 2.3783 - val_acc: 0.6475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12219eb38>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor = ['val_loss'], min_delta = 0.01, \n",
    "                              patience = 5, verbose = 1, mode = 'auto')\n",
    "\n",
    "# training\n",
    "model.fit(train_padding, train_y,\n",
    "          batch_size = 256,\n",
    "          epochs = 40,\n",
    "          shuffle = True,\n",
    "          validation_data=(test_padding, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# model.save('fastText2_model_final.h5')\n",
    "# model = load_model('./fastText2_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "train_preds = model.predict(train_padding)\n",
    "test_preds = model.predict(test_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics (relaxed, nonrelaxed & bar-chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a map that contains empathy and its corresponding polarity score, this map will be used to compute metrics\n",
    "dict_polarities = {}\n",
    "for i in range(empathies.shape[0]):\n",
    "    dict_polarities[empathies.iloc[i][0]] = float(empathies.iloc[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to compare against the ground truth in a **relaxed**  format, which means if the predicted class is not exactly the same as the truth, but has the same polarity score. The prediction will still be counted as truth positive\n",
    "\n",
    "The output is a dictionary contained label: number of true positives, false positives and false negatives\n",
    "the output will be used for calculating overall precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_truth_relaxed(cutoff_value, dict_polarities, model, inputs, all_preds, start, stop):\n",
    "    '''\n",
    "    output: {label: [TP, FP, FN]}\n",
    "    '''\n",
    "    label_s = list(dict_polarities.keys())\n",
    "    res = {}\n",
    "    \n",
    "    for l in label_s:\n",
    "        # True Positives, False Positives, False Negatives\n",
    "        res[l] = [0, 0, 0]\n",
    "    \n",
    "    j = -1\n",
    "    for i in range(start, stop):\n",
    "        j += 1\n",
    "        preds = all_preds[j]\n",
    "        true_scores = [dict_polarities[la.strip()] for la in msg.iloc[i]['empathy'].split(',') if la != '']\n",
    "        \n",
    "        # store predicted labels and the corresponding polarity score\n",
    "        pred_labs, scores  = [], []\n",
    "        preds = [list(empathies['empathy'][np.argsort(preds)[63:58:-1]]), np.sort(preds)[63:58:-1]]\n",
    "        \n",
    "        for lab, p in zip(preds[0], preds[1]):\n",
    "            if p > cutoff_value:\n",
    "                pred_labs.append(lab)\n",
    "                scores.append(dict_polarities[lab])\n",
    "                if lab in msg.iloc[i]['empathy'] or dict_polarities[lab] in true_scores:\n",
    "                    # true positive\n",
    "                    res[lab][0] += 1\n",
    "                if lab not in msg.iloc[i]['empathy'] and dict_polarities[lab] not in true_scores:\n",
    "                    # false positive\n",
    "                    res[lab][1] += 1\n",
    "                \n",
    "        for true_lab in msg.iloc[i]['empathy'].split(','):\n",
    "            true_lab = true_lab.strip()\n",
    "            if true_lab != '' and true_lab != ' ' and true_lab not in pred_labs and dict_polarities[lab] not in scores:\n",
    "                # false negative\n",
    "                res[true_lab][2] += 1\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiar as the above function. The following function is used to compare against the ground truth, but in a **nonrelaxed** format -- the predicted class has to be exactly the same as the truth in order to be counted as true positive.\n",
    "\n",
    "The output is a dictionary contained label: number of true positives, false positives and false negatives\n",
    "the output will be used for calculating overall precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_truth_nonrelaxed(cutoff_value, dict_polarities, model, inputs, all_preds, start, stop):\n",
    "    '''\n",
    "    output: {label: [TP, FP, FN]}\n",
    "    '''\n",
    "    label_s = list(dict_polarities.keys())\n",
    "    res = {}\n",
    "    \n",
    "    for l in label_s:\n",
    "        # True Positives, False Positives, False Negatives\n",
    "        res[l] = [0, 0, 0]\n",
    "    \n",
    "    j = -1\n",
    "    for i in range(start, stop):\n",
    "        j += 1\n",
    "        preds = all_preds[j]\n",
    "        true_scores = [dict_polarities[la.strip()] for la in msg.iloc[i]['empathy'].split(',') if la != '']\n",
    "        \n",
    "        # store predicted labels and the corresponding polarity score\n",
    "        pred_labs, scores  = [], []\n",
    "        preds = [list(empathies['empathy'][np.argsort(preds)[63:58:-1]]), np.sort(preds)[63:58:-1]]\n",
    "        \n",
    "        for lab, p in zip(preds[0], preds[1]):\n",
    "            if p > cutoff_value:\n",
    "                pred_labs.append(lab)\n",
    "                scores.append(dict_polarities[lab])\n",
    "                if lab in msg.iloc[i]['empathy']:\n",
    "                    # true positive\n",
    "                    res[lab][0] += 1\n",
    "                if lab not in msg.iloc[i]['empathy']:\n",
    "                    # false positive\n",
    "                    res[lab][1] += 1\n",
    "                \n",
    "        for true_lab in msg.iloc[i]['empathy'].split(','):\n",
    "            true_lab = true_lab.strip()\n",
    "            if true_lab != '' and true_lab != ' ' and true_lab not in pred_labs:\n",
    "                # false negative\n",
    "                res[true_lab][2] += 1\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overall_score(res):\n",
    "    '''\n",
    "    output precision, recall and F1 score\n",
    "    '''\n",
    "    num_TP, num_FP, num_FN = 0, 0, 0\n",
    "    \n",
    "    for _, vals in res.items():\n",
    "        num_TP += vals[0]\n",
    "        num_FP += vals[1]\n",
    "        num_FN += vals[2]\n",
    "            \n",
    "    precision = num_TP/(num_TP+num_FP)\n",
    "    recall = num_TP/(num_TP+num_FN)\n",
    "    \n",
    "    return precision, recall, 2/(1/precision + 1/recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the best cutoff value with the training data\n",
    "ps, rs, fs = [], [], []\n",
    "for proposed_cut in np.arange(.0, .5, .01):\n",
    "    res = compare_truth_relaxed(proposed_cut, dict_polarities, model, msg, train_preds, 0, 2500)\n",
    "    p, r, f = overall_score(res)\n",
    "    ps.append(p)\n",
    "    rs.append(rs)\n",
    "    fs.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8YuIFBklfhGRIvP/AcD5KncwzGpsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12203cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the change of F1 score as the cutoff value increases from 0.0 to 0.5\n",
    "proposed_cuts = np.arange(.0, .5, .01)\n",
    "plt.plot(proposed_cuts, fs, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the best cutoff value\n",
    "max_fs_idx = fs.index(max(fs))\n",
    "np.arange(.0, .5, .01)[max_fs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate bar-chart of true positives, false positives and false negatives for each class\n",
    "def confu_plot(con_matrix):\n",
    "    '''\n",
    "    generate bar-chart of true positives, false positives and false negatives for each class\n",
    "    '''\n",
    "    names = []\n",
    "    TP, FP, FN = [], [], []\n",
    "    for k, vals in con_matrix.items():\n",
    "        name=k.replace('__label__', '')\n",
    "        names.append(name)\n",
    "        TP.append(vals[0])\n",
    "        FP.append(vals[1])\n",
    "        FN.append(vals[2])\n",
    "\n",
    "    trace1 = go.Bar(\n",
    "        x=names,\n",
    "        y=TP,\n",
    "        name='True Positive'\n",
    "    )\n",
    "\n",
    "    trace2 = go.Bar(\n",
    "        x=names,\n",
    "        y=FP,\n",
    "        name='False Positive'\n",
    "    )\n",
    "\n",
    "    trace3 = go.Bar(\n",
    "        x=names,\n",
    "        y=FN,\n",
    "        name='False Negative'\n",
    "    )\n",
    "\n",
    "    data = [trace1, trace2, trace3]\n",
    "    layout = go.Layout(\n",
    "        barmode='group',\n",
    "\n",
    "        xaxis=dict(\n",
    "            title='labels',\n",
    "            titlefont=dict(\n",
    "                size=18,\n",
    "            ),\n",
    "            showticklabels=True,\n",
    "            tickangle=45,\n",
    "            tickfont=dict(\n",
    "                size=14,\n",
    "                color='black'\n",
    "            ),\n",
    "            exponentformat='e',\n",
    "            showexponent='All'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    plotly.offline.iplot(fig, filename='grouped-bar') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance on the validation dataset is: \n",
      "precision: 0.8164652567975831\n",
      "recall: 0.7672107877927609\n",
      "F1: 0.7910720819612148\n"
     ]
    }
   ],
   "source": [
    "# calculate overall precision, recall and F1 score of validation data with the cutoff value found from above\n",
    "res = compare_truth_relaxed(.16, dict_polarities, model, msg, test_preds, 2500, msg.shape[0])\n",
    "overall_scores = overall_score(res)\n",
    "\n",
    "print('model performance on the validation dataset is: ')\n",
    "print('precision: {}'.format(overall_scores[0]))\n",
    "print('recall: {}'.format(overall_scores[1]))\n",
    "print('F1: {}'.format(overall_scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"5162e636-9007-488c-9cd8-14cef791dc1e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5162e636-9007-488c-9cd8-14cef791dc1e\", [{\"type\": \"bar\", \"y\": [21, 36, 7, 11, 33, 28, 10, 8, 9, 24, 3, 0, 3, 9, 73, 10, 20, 5, 20, 44, 31, 4, 11, 12, 3, 12, 10, 9, 45, 0, 2, 17, 1, 4, 0, 12, 23, 11, 21, 18, 0, 15, 2, 2, 4, 14, 8, 8, 21, 41, 27, 31, 36, 1, 1, 74, 17, 0, 61, 53, 0, 41, 0, 4], \"x\": [\"unmotivated\", \"sleep\", \"uneasy\", \"unsatisfied\", \"emotionless\", \"average\", \"relief\", \"good\", \"sick\", \"stress\", \"upset\", \"frustrated\", \"inspired\", \"vulnerable\", \"happy\", \"loss\", \"determined\", \"sexy\", \"idk\", \"depressed\", \"pain\", \"despair\", \"intoxicated\", \"hungry\", \"hopeful\", \"engaged\", \"full\", \"affectionate\", \"calm\", \"surprised\", \"creative\", \"distortion\", \"loved\", \"restless\", \"ignored\", \"great\", \"confused\", \"lazy\", \"annoyed\", \"disconnected\", \"panic\", \"pensive\", \"regret\", \"uncertain\", \"embarrassed\", \"scared\", \"emotional\", \"stable\", \"better\", \"angry\", \"lonely\", \"bored\", \"okay\", \"playful\", \"rude\", \"sad\", \"confidence\", \"healthy\", \"tired\", \"anxious\", \"thirsty\", \"excited\", \"jealous\", \"flustered\"], \"name\": \"True Positive\"}, {\"type\": \"bar\", \"y\": [10, 3, 1, 4, 11, 13, 1, 5, 4, 5, 3, 0, 1, 0, 14, 0, 1, 0, 15, 7, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0, 0, 4, 0, 0, 0, 8, 2, 0, 10, 15, 0, 4, 0, 2, 4, 1, 1, 0, 6, 4, 2, 0, 27, 0, 0, 21, 2, 0, 9, 10, 0, 1, 0, 1], \"x\": [\"unmotivated\", \"sleep\", \"uneasy\", \"unsatisfied\", \"emotionless\", \"average\", \"relief\", \"good\", \"sick\", \"stress\", \"upset\", \"frustrated\", \"inspired\", \"vulnerable\", \"happy\", \"loss\", \"determined\", \"sexy\", \"idk\", \"depressed\", \"pain\", \"despair\", \"intoxicated\", \"hungry\", \"hopeful\", \"engaged\", \"full\", \"affectionate\", \"calm\", \"surprised\", \"creative\", \"distortion\", \"loved\", \"restless\", \"ignored\", \"great\", \"confused\", \"lazy\", \"annoyed\", \"disconnected\", \"panic\", \"pensive\", \"regret\", \"uncertain\", \"embarrassed\", \"scared\", \"emotional\", \"stable\", \"better\", \"angry\", \"lonely\", \"bored\", \"okay\", \"playful\", \"rude\", \"sad\", \"confidence\", \"healthy\", \"tired\", \"anxious\", \"thirsty\", \"excited\", \"jealous\", \"flustered\"], \"name\": \"False Positive\"}, {\"type\": \"bar\", \"y\": [5, 13, 7, 2, 5, 4, 5, 11, 5, 4, 7, 0, 1, 1, 15, 5, 7, 3, 6, 7, 10, 5, 1, 0, 0, 6, 0, 2, 19, 2, 0, 8, 1, 0, 0, 4, 9, 5, 11, 13, 0, 0, 4, 6, 6, 0, 3, 4, 1, 6, 10, 10, 11, 0, 0, 26, 4, 0, 14, 10, 0, 3, 0, 1], \"x\": [\"unmotivated\", \"sleep\", \"uneasy\", \"unsatisfied\", \"emotionless\", \"average\", \"relief\", \"good\", \"sick\", \"stress\", \"upset\", \"frustrated\", \"inspired\", \"vulnerable\", \"happy\", \"loss\", \"determined\", \"sexy\", \"idk\", \"depressed\", \"pain\", \"despair\", \"intoxicated\", \"hungry\", \"hopeful\", \"engaged\", \"full\", \"affectionate\", \"calm\", \"surprised\", \"creative\", \"distortion\", \"loved\", \"restless\", \"ignored\", \"great\", \"confused\", \"lazy\", \"annoyed\", \"disconnected\", \"panic\", \"pensive\", \"regret\", \"uncertain\", \"embarrassed\", \"scared\", \"emotional\", \"stable\", \"better\", \"angry\", \"lonely\", \"bored\", \"okay\", \"playful\", \"rude\", \"sad\", \"confidence\", \"healthy\", \"tired\", \"anxious\", \"thirsty\", \"excited\", \"jealous\", \"flustered\"], \"name\": \"False Negative\"}], {\"xaxis\": {\"titlefont\": {\"size\": 18}, \"exponentformat\": \"e\", \"tickfont\": {\"size\": 14, \"color\": \"black\"}, \"showexponent\": \"All\", \"showticklabels\": true, \"tickangle\": 45, \"title\": \"labels\"}, \"barmode\": \"group\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "confu_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the best cutoff value with the training data using the non-relaxed comparison function\n",
    "ps, rs, fs = [], [], []\n",
    "for proposed_cut in np.arange(.0, .5, .01):\n",
    "    res = compare_truth_nonrelaxed(proposed_cut, dict_polarities, model, msg, train_preds, 0, 2500)\n",
    "    p, r, f = overall_score(res)\n",
    "    ps.append(p)\n",
    "    rs.append(rs)\n",
    "    fs.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125b1a400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the change of F1 score as the cutoff value increases from 0.0 to 0.5\n",
    "proposed_cuts = np.arange(.0, .5, .01)\n",
    "# red dashes\n",
    "plt.plot(proposed_cuts, fs, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14000000000000001"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the best cutoff value\n",
    "max_fs_idx = fs.index(max(fs))\n",
    "np.arange(.0, .5, .01)[max_fs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance using non-relaxed comparison function on the validation dataset is: \n",
      "precision: 0.7674247982391783\n",
      "recall: 0.7263888888888889\n",
      "F1: 0.7463432037103104\n"
     ]
    }
   ],
   "source": [
    "# calculate overall precision, recall and F1 score of validation data with the cutoff value found from above\n",
    "res = compare_truth_nonrelaxed(.14, dict_polarities, model, msg, test_preds, 2500, msg.shape[0])\n",
    "overall_scores = overall_score(res)\n",
    "\n",
    "print('model performance using non-relaxed comparison function on the validation dataset is: ')\n",
    "print('precision: {}'.format(overall_scores[0]))\n",
    "print('recall: {}'.format(overall_scores[1]))\n",
    "print('F1: {}'.format(overall_scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"1d242f14-045a-4161-8797-a0c067eb5fad\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1d242f14-045a-4161-8797-a0c067eb5fad\", [{\"type\": \"bar\", \"y\": [19, 37, 7, 11, 32, 30, 10, 9, 9, 24, 3, 0, 3, 9, 69, 10, 21, 5, 19, 38, 31, 4, 11, 12, 3, 10, 10, 9, 46, 0, 2, 14, 1, 4, 0, 10, 22, 11, 20, 18, 0, 15, 2, 2, 4, 14, 7, 8, 21, 37, 29, 31, 35, 1, 1, 67, 17, 0, 60, 48, 0, 40, 0, 4], \"x\": [\"unmotivated\", \"sleep\", \"uneasy\", \"unsatisfied\", \"emotionless\", \"average\", \"relief\", \"good\", \"sick\", \"stress\", \"upset\", \"frustrated\", \"inspired\", \"vulnerable\", \"happy\", \"loss\", \"determined\", \"sexy\", \"idk\", \"depressed\", \"pain\", \"despair\", \"intoxicated\", \"hungry\", \"hopeful\", \"engaged\", \"full\", \"affectionate\", \"calm\", \"surprised\", \"creative\", \"distortion\", \"loved\", \"restless\", \"ignored\", \"great\", \"confused\", \"lazy\", \"annoyed\", \"disconnected\", \"panic\", \"pensive\", \"regret\", \"uncertain\", \"embarrassed\", \"scared\", \"emotional\", \"stable\", \"better\", \"angry\", \"lonely\", \"bored\", \"okay\", \"playful\", \"rude\", \"sad\", \"confidence\", \"healthy\", \"tired\", \"anxious\", \"thirsty\", \"excited\", \"jealous\", \"flustered\"], \"name\": \"True Positive\"}, {\"type\": \"bar\", \"y\": [12, 3, 1, 4, 15, 14, 1, 5, 6, 7, 3, 0, 1, 0, 23, 0, 2, 0, 16, 14, 0, 2, 1, 0, 1, 4, 1, 2, 6, 0, 0, 8, 0, 0, 0, 10, 3, 0, 11, 16, 0, 4, 0, 2, 4, 1, 3, 0, 7, 8, 2, 0, 33, 0, 0, 29, 2, 0, 11, 16, 0, 2, 0, 1], \"x\": [\"unmotivated\", \"sleep\", \"uneasy\", \"unsatisfied\", \"emotionless\", \"average\", \"relief\", \"good\", \"sick\", \"stress\", \"upset\", \"frustrated\", \"inspired\", \"vulnerable\", \"happy\", \"loss\", \"determined\", \"sexy\", \"idk\", \"depressed\", \"pain\", \"despair\", \"intoxicated\", \"hungry\", \"hopeful\", \"engaged\", \"full\", \"affectionate\", \"calm\", \"surprised\", \"creative\", \"distortion\", \"loved\", \"restless\", \"ignored\", \"great\", \"confused\", \"lazy\", \"annoyed\", \"disconnected\", \"panic\", \"pensive\", \"regret\", \"uncertain\", \"embarrassed\", \"scared\", \"emotional\", \"stable\", \"better\", \"angry\", \"lonely\", \"bored\", \"okay\", \"playful\", \"rude\", \"sad\", \"confidence\", \"healthy\", \"tired\", \"anxious\", \"thirsty\", \"excited\", \"jealous\", \"flustered\"], \"name\": \"False Positive\"}, {\"type\": \"bar\", \"y\": [7, 12, 9, 4, 6, 3, 6, 10, 5, 8, 9, 0, 1, 2, 14, 6, 9, 3, 6, 11, 15, 8, 1, 0, 1, 6, 1, 3, 17, 2, 0, 7, 1, 0, 2, 4, 10, 5, 20, 14, 0, 2, 6, 7, 7, 1, 3, 4, 1, 6, 11, 11, 13, 0, 0, 29, 5, 0, 19, 12, 0, 8, 0, 1], \"x\": [\"unmotivated\", \"sleep\", \"uneasy\", \"unsatisfied\", \"emotionless\", \"average\", \"relief\", \"good\", \"sick\", \"stress\", \"upset\", \"frustrated\", \"inspired\", \"vulnerable\", \"happy\", \"loss\", \"determined\", \"sexy\", \"idk\", \"depressed\", \"pain\", \"despair\", \"intoxicated\", \"hungry\", \"hopeful\", \"engaged\", \"full\", \"affectionate\", \"calm\", \"surprised\", \"creative\", \"distortion\", \"loved\", \"restless\", \"ignored\", \"great\", \"confused\", \"lazy\", \"annoyed\", \"disconnected\", \"panic\", \"pensive\", \"regret\", \"uncertain\", \"embarrassed\", \"scared\", \"emotional\", \"stable\", \"better\", \"angry\", \"lonely\", \"bored\", \"okay\", \"playful\", \"rude\", \"sad\", \"confidence\", \"healthy\", \"tired\", \"anxious\", \"thirsty\", \"excited\", \"jealous\", \"flustered\"], \"name\": \"False Negative\"}], {\"xaxis\": {\"titlefont\": {\"size\": 18}, \"exponentformat\": \"e\", \"tickfont\": {\"size\": 14, \"color\": \"black\"}, \"showexponent\": \"All\", \"showticklabels\": true, \"tickangle\": 45, \"title\": \"labels\"}, \"barmode\": \"group\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "confu_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# used for transform a string into the acceptable format for the model\n",
    "def text_processor(input_str, token_indices, max_len = 128):\n",
    "    '''\n",
    "    ---\n",
    "    args:\n",
    "        input_str: an input string\n",
    "        token_indices: map of token index calculated from above\n",
    "        max_len: the length of input that the model accepts\n",
    "    ---\n",
    "    return:\n",
    "       padded sequence for the model \n",
    "    '''\n",
    "    tokens = TweetTokenizer().tokenize(input_str.lower())\n",
    "    sen2input = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if i < max_len:\n",
    "            sen2input.append(token_indices[token])\n",
    "            \n",
    "    sen2input = add_ngram([sen2input], token_indice2, 3)\n",
    "    \n",
    "    return pad_sequences(sen2input, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model performance on emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hungry', 'okay', 'sad', 'tired', 'happy']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_input = text_processor('😋', token_indices)\n",
    "random_res = model.predict(random_input)\n",
    "# print out top five predictions\n",
    "list(empathies['empathy'][np.argsort(random_res[0])[63:58:-1]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
